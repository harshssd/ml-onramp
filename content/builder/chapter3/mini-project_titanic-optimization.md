---
id: ai-builder-ch3-mini-project
title: Titanic Model Optimization Challenge
type: mini-project
duration_min: 90
prereqs: ["ai-builder-ch3-lesson5"]
tags: ["intermediate","mini-project","optimization","titanic","model-comparison"]
widgets:
  - type: "dataset_explorer"
    data: "/widgets/dataset_explorer.titanic.json"
  - type: "model_comparison_dashboard"
    data: "/widgets/model_comparison.titanic.json"
  - type: "hyperparam_tuning_playground"
    data: "/widgets/hyperparam_tuning.titanic.json"
  - type: "cv_visualizer"
    data: "/widgets/cv_visualizer.titanic.json"
  - type: "performance_tradeoff_explorer"
    data: "/widgets/performance_tradeoff.titanic.json"
goals:
  - Build the best possible Titanic survival predictor
  - Experiment with different model architectures
  - Master hyperparameter tuning techniques
  - Apply cross-validation strategies
  - Understand performance trade-offs
badge: "Optimization Master ü•á"
completion_criteria:
  - Train at least 3 different model architectures
  - Tune hyperparameters for at least 2 models
  - Use cross-validation to validate performance
  - Submit results with written reflection
  - Achieve minimum 80% accuracy on validation set
---

# üèÜ Mini-Project: Titanic Model Optimization Challenge

## üéØ Goal
Build the **best possible Titanic survival predictor** by experimenting with **different model architectures, hyperparameter tuning, and cross-validation strategies**.

## üìÇ Dataset
- **Titanic dataset** (demo CSV with ~24 rows for widget play + larger version available for export)
- **Features**: `Age`, `Sex`, `Pclass`, `Fare`, `Embarked`, `SibSp`, `Parch`
- **Target**: `Survived` (0 = No, 1 = Yes)
- **Challenge**: Predict passenger survival based on available features

## üìñ Project Steps

### Step 1: Baseline Model with Cross-Validation
**Objective**: Establish a performance baseline with CV stability

1. **Load the Dataset**
   - Use the **Dataset Explorer** to inspect Titanic features
   - Check for missing values, data types, and distributions
   - Note any data quality issues and class imbalance

2. **Train Baseline Model**
   - Train a **Logistic Regression** with default hyperparameters
   - Use **5-fold Cross-Validation** to evaluate stability
   - Record average metrics: Accuracy, Precision, Recall, F1
   - Note standard deviation across folds

3. **Document Results**
   - Save baseline performance metrics with CV statistics
   - Note training time, model complexity, and fold variance
   - Use **CV Visualizer** to see fold-by-fold performance

### Step 2: Competing Architectures with CV
**Objective**: Compare different model types with cross-validation

1. **Random Forest**
   - Train with varied `max_depth` (3, 5, 10) and `n_estimators` (50, 100, 200)
   - Use **5-fold Stratified CV** to evaluate stability
   - Note feature importance rankings and fold variance

2. **Gradient Boosting**
   - Try XGBoost/LightGBM with different learning rates
   - Apply **stratified k-fold CV** for fair comparison
   - Compare with other tree-based methods

3. **Neural Network**
   - Train a small neural network (2 hidden layers, 50 neurons each)
   - Experiment with different architectures
   - Use **CV Visualizer** to check fold stability

4. **Compare All Models with CV**
   - Use the **Model Comparison Dashboard** to visualize results
   - Use **CV Visualizer** to compare stability across folds
   - Identify the best-performing and most stable architecture

### Step 3: Hyperparameter Tuning + CV Integration
**Objective**: Optimize model performance with cross-validation

1. **Select Models for Tuning**
   - Choose the 2 best-performing models from Step 2
   - Focus on models with good potential for improvement

2. **Use Hyperparameter Tuning Playground with CV**
   - **Logistic Regression**: Tune `C` (regularization strength) with stratified CV
   - **Random Forest**: Tune `max_depth`, `n_estimators` with stratified CV
   - **Neural Network**: Tune `learning_rate`, `hidden_layer_sizes` with stratified CV
   - **Gradient Boosting**: Tune `learning_rate`, `n_estimators` with stratified CV

3. **Compare Search Methods with CV**
   - Try **Grid Search** with 5-fold stratified CV
   - Use **Random Search** with stratified CV for efficiency
   - Experiment with **Bayesian Optimization** with CV for smart search

4. **Document Improvements with CV**
   - Record performance gains from tuning with CV statistics
   - Note which hyperparameters had the biggest impact
   - Use **CV Visualizer** to see fold stability improvements

### Step 4: Cross-Validation Strategy Exploration
**Objective**: Explore different CV strategies and validate model stability

1. **Compare CV Strategies**
   - Switch between **k-fold** and **stratified CV** for Titanic (imbalanced survival rate)
   - Use **CV Visualizer** to see the difference in fold distributions
   - Note how stratified CV affects performance estimates

2. **Time-Series CV Experiment**
   - Run **time-series CV** on simulated sequential Titanic boarding data
   - See how temporal ordering affects model performance
   - Compare with random CV results

3. **Analyze CV Results**
   - Look for signs of overfitting or underfitting across folds
   - Compare mean performance vs standard deviation
   - Use **CV Visualizer** to identify the most stable model

4. **Fold Variance Analysis**
   - Check which models have consistent performance across folds
   - Identify models that fluctuate heavily
   - Choose the most stable and reliable model

### Step 5: Final Optimization
**Objective**: Apply advanced optimization techniques

1. **Regularization**
   - Apply L1/L2 regularization to prevent overfitting
   - Experiment with different regularization strengths
   - Monitor validation performance

2. **Early Stopping**
   - For neural networks, implement early stopping
   - Monitor training vs validation loss
   - Stop training when validation performance plateaus

3. **Model Compression**
   - Try feature selection to reduce model complexity
   - Experiment with model pruning if available
   - Balance accuracy vs interpretability

4. **Performance Trade-offs**
   - Use the **Performance Trade-off Explorer** to analyze:
     - Accuracy vs Training Time
     - Accuracy vs Model Size
     - Accuracy vs Interpretability
     - Accuracy vs Inference Speed

## üß© Interactive Components

### Dataset Explorer
- **Purpose**: Inspect Titanic features and data quality
- **Features**: Missing value analysis, data type checking, distribution plots
- **Usage**: Understand your data before modeling

### Model Comparison Dashboard
- **Purpose**: Compare different model architectures
- **Features**: Performance metrics, training time, interpretability scores
- **Usage**: Identify the best model type for your problem

### Hyperparameter Tuning Playground (with CV Integration)
- **Purpose**: Optimize model hyperparameters with cross-validation
- **Features**: Grid search, random search, Bayesian optimization with stratified CV
- **Usage**: Find the best hyperparameter settings with reliable validation
- **CV Integration**: All tuning uses 5-fold stratified CV for robust evaluation

### Cross-Validation Visualizer
- **Purpose**: Validate model stability across folds and compare CV strategies
- **Features**: CV score distributions, fold-by-fold analysis, strategy comparison
- **Usage**: Ensure your model generalizes well and choose the right CV strategy
- **Strategies**: K-fold, Stratified K-fold, Time-series CV, Leave-One-Out CV

### Performance Trade-off Explorer
- **Purpose**: Analyze performance trade-offs
- **Features**: Accuracy vs speed, accuracy vs complexity, CV stability analysis
- **Usage**: Make informed decisions about model selection considering stability

## üîß Enhanced Widget Configurations

### CV Visualizer Configuration
```json
{
  "widget": "cv_visualizer",
  "dataset": "titanic.csv",
  "models": ["LogisticRegression", "RandomForest", "GradientBoosting", "NeuralNetwork"],
  "cv_strategies": ["kfold", "stratified", "timeseries"],
  "folds": [3, 5, 10],
  "metrics": ["accuracy", "precision", "recall", "f1"],
  "visualization": {
    "show_fold_scores": true,
    "show_fold_variance": true,
    "show_fold_distributions": true,
    "show_performance_heatmap": true
  }
}
```

### Hyperparameter Tuning Playground (with CV)
```json
{
  "widget": "hyperparam_tuning_playground",
  "dataset": "titanic.csv",
  "models": ["LogisticRegression", "RandomForest", "GradientBoosting", "NeuralNetwork"],
  "search_methods": ["grid", "random", "bayesian"],
  "cv_strategy": "stratified",
  "cv_folds": 5,
  "parameters": {
    "LogisticRegression": {"C": [0.01, 0.1, 1, 10]},
    "RandomForest": {"max_depth": [3, 5, 10], "n_estimators": [50, 100, 200]},
    "GradientBoosting": {"learning_rate": [0.01, 0.1, 0.2], "n_estimators": [50, 100, 200]},
    "NeuralNetwork": {"hidden_layer_sizes": ["(50,)", "(100,)", "(50, 50)"], "learning_rate": [0.001, 0.01, 0.1]}
  },
  "metrics": ["accuracy", "f1", "training_time", "cv_std"]
}
```

## üìä Deliverables

### 1. Final Tuned Model with CV Results
- **Best performing model** with optimized hyperparameters
- **Cross-validation performance** with mean and standard deviation
- **Model configuration** (hyperparameters, architecture, CV strategy used)
- **Stability metrics** (fold variance, consistency across folds)

### 2. Cross-Validation Analysis
- **CV Strategy Comparison**: Results from k-fold vs stratified vs time-series CV
- **Fold Stability Analysis**: Which models were most consistent across folds
- **CV Visualizer Screenshots**: Showing fold distributions and performance heatmaps
- **Stratified CV Impact**: How class balance affected performance estimates

### 3. Written Reflection
**Answer these questions in 2-3 paragraphs:**

1. **Model Selection**: Which model architecture performed best and why?
2. **Hyperparameter Impact**: Which hyperparameters had the biggest impact on performance?
3. **CV Strategy**: Which cross-validation strategy worked best for the Titanic dataset and why?
4. **Stability Analysis**: How stable were your results across different folds? Which models were most reliable?
5. **Trade-offs**: What trade-offs did you make between accuracy, interpretability, computational cost, and stability?
6. **Lessons Learned**: What would you do differently next time?

### 4. Visualization Screenshots
- **Model Comparison Dashboard** showing all models with CV metrics
- **Hyperparameter Tuning Results** with optimization curves and CV integration
- **Cross-Validation Plots** showing fold stability and strategy comparison
- **CV Visualizer Screenshots** showing fold distributions and performance heatmaps
- **Performance Trade-off Charts** including stability considerations

### 5. Enhanced Performance Summary
Create a comprehensive table with your final results:

| Model | CV Strategy | Mean Accuracy | CV Std | Precision | Recall | F1 | Training Time | Stability Score |
|-------|-------------|---------------|--------|-----------|--------|----|--------------|-----------------|
| Baseline LR | Stratified 5-fold | | | | | | | |
| Tuned LR | Stratified 5-fold | | | | | | | |
| Random Forest | Stratified 5-fold | | | | | | | |
| Gradient Boosting | Stratified 5-fold | | | | | | | |
| Neural Network | Stratified 5-fold | | | | | | | |
| **Best Model** | | | | | | | | |

### 6. CV Strategy Comparison
Document your findings from different CV strategies:

| CV Strategy | Mean Accuracy | CV Std | Best For | Notes |
|-------------|---------------|--------|----------|-------|
| K-fold | | | | |
| Stratified K-fold | | | | |
| Time-series | | | | |

## üèÖ Badge Awarded

**Optimization Master ü•á**

**Unlock Requirements:**
- ‚úÖ Train at least 3 different model architectures
- ‚úÖ Tune hyperparameters for at least 2 models
- ‚úÖ Use cross-validation to validate performance
- ‚úÖ Submit results with written reflection
- ‚úÖ Achieve minimum 80% accuracy on validation set

## üéØ Success Criteria

### Minimum Requirements
- **Accuracy**: ‚â• 80% mean accuracy with stratified 5-fold CV
- **Models**: At least 3 different architectures tested with CV
- **Tuning**: Hyperparameters optimized for at least 2 models using CV
- **CV Strategies**: Compare at least 2 different CV strategies (k-fold vs stratified)
- **Stability**: CV standard deviation ‚â§ 5% for best model
- **Documentation**: Complete reflection and CV analysis submitted

### Excellence Criteria
- **Accuracy**: ‚â• 85% mean accuracy with stratified 5-fold CV
- **Comprehensive**: All 5 model types tested and compared with CV
- **Advanced Tuning**: Bayesian optimization used with CV integration
- **CV Exploration**: Test k-fold, stratified, and time-series CV strategies
- **Stability Analysis**: Detailed fold variance analysis and stability metrics
- **Thorough Analysis**: Detailed trade-off analysis including stability considerations
- **Insights**: Clear understanding of model behavior and CV strategy impact

## üí° Tips for Success

1. **Start Simple**: Begin with baseline models and simple CV before complex tuning
2. **Use Stratified CV**: Essential for imbalanced datasets like Titanic survival
3. **Compare CV Strategies**: Test different CV approaches to understand their impact
4. **Monitor Stability**: Check fold variance to identify reliable models
5. **Integrate CV in Tuning**: Use CV during hyperparameter optimization, not just evaluation
6. **Document Everything**: Keep track of all experiments, CV results, and stability metrics
7. **Think About Trade-offs**: Consider accuracy, stability, interpretability, and computational cost
8. **Validate Thoroughly**: Use multiple CV strategies to ensure robust evaluation
9. **Reflect Deeply**: Think about why certain approaches worked better and CV strategy impact
10. **Focus on Stability**: A stable model with slightly lower accuracy is often better than an unstable one

## üîó Resources

- [Scikit-learn Model Selection Guide](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
- [Hyperparameter Tuning Best Practices](https://scikit-learn.org/stable/modules/grid_search.html)
- [Cross-Validation Strategies](https://scikit-learn.org/stable/modules/cross_validation.html)
- [Titanic Dataset Documentation](https://www.kaggle.com/c/titanic/data)

---

**Good luck with your optimization challenge! üöÄ**
